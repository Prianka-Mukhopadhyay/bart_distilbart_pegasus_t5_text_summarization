{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPskvbfFFla23xHWvNpJ79v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prianka-Mukhopadhyay/bart_distilbart_pegasus_t5_text_summarization/blob/main/bart_distilbart_pegasus_t5_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zgTOI2kL0M-1"
      },
      "outputs": [],
      "source": [
        "#installing everything necessary\n",
        "!pip install transformers torch gradio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing BART"
      ],
      "metadata": {
        "id": "dnP9qL1O5vQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Load the model\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zZLkqVX51HIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def summarize_text(text):\n",
        "#     # Encode the text into tokens\n",
        "#     inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "#     # Generate summary IDs\n",
        "#     summary_ids = model.generate(\n",
        "#         inputs['input_ids'],\n",
        "#         max_length=150,\n",
        "#         min_length=40,\n",
        "#         length_penalty=2.0,\n",
        "#         num_beams=4,\n",
        "#         early_stopping=True\n",
        "#     )\n",
        "\n",
        "#     # Decode the generated IDs back into text\n",
        "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "#     return summary\n"
      ],
      "metadata": {
        "id": "jDdHDUyJF1fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text):\n",
        "    # Encode the text into tokens\n",
        "    inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "    # Generate summary IDs\n",
        "    summary_ids = model.generate(\n",
        "      inputs['input_ids'],\n",
        "      max_length=100,   # shorter summaries\n",
        "      min_length=20,\n",
        "      length_penalty=1.0,\n",
        "      num_beams=6,      # higher beam search for better results\n",
        "      early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the generated IDs back into text\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "4fOa-z9_MYty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"\n",
        "My favorite book in the Harry Potter series is \"Harry Potter and the Prisoner of Azkaban\". It's a pivotal book in the series, introducing darker themes and complex characters while still maintaining the whimsical charm of the earlier books. The introduction of Sirius Black and Remus Lupin, along with the concept of dementors, adds depth to the wizarding world and Harry's personal journey. The intricate plot, with its time-travel elements and moral ambiguities, keeps the reader guessing until the very end. It's a book that truly showcases J.K. Rowling's storytelling prowess and her ability to blend fantasy with relatable human emotions\n",
        "\"\"\"\n",
        "print(summarize_text(sample_text))"
      ],
      "metadata": {
        "id": "NYlvjCutMqAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio"
      ],
      "metadata": {
        "id": "w0E4QQd-JjrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "diusSsxbG6n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=summarize_text,           # Function to call\n",
        "    inputs=gr.Textbox(lines=10, placeholder=\"Paste text here...\"),  # Input box\n",
        "    outputs=\"text\",              # Output is plain text\n",
        "    title=\"BART Text Summarizer\",\n",
        "    description=\"Enter text and get a summary using Hugging Face BART model.\"\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ogrwQ7wUJiQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load DistilBART and Tokenizer**"
      ],
      "metadata": {
        "id": "i1vgMlazSksR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# DistilBART is a smaller, faster version of BART.\n",
        "# Model: sshleifer/distilbart-cnn-12-6\n",
        "# Tokenizer converts text into numerical tokens the model can understand.\n",
        "\n",
        "distilbart_model_name = \"sshleifer/distilbart-cnn-12-6\"\n",
        "distilbart_tokenizer = BartTokenizer.from_pretrained(distilbart_model_name)\n",
        "distilbart_model = BartForConditionalGeneration.from_pretrained(distilbart_model_name)"
      ],
      "metadata": {
        "id": "qLG69tH8SkT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Summary with DistilBART**"
      ],
      "metadata": {
        "id": "XZr_KxX5TBXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_distilbart(text):\n",
        "    # Encode the input text into tokens\n",
        "    inputs = distilbart_tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "    # Generate summary IDs with beam search\n",
        "    summary_ids = distilbart_model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=150,        # max length of summary\n",
        "        min_length=40,         # minimum length\n",
        "        length_penalty=2.0,    # encourages more concise summary\n",
        "        num_beams=4,           # beam search size for better quality\n",
        "        early_stopping=True    # stop generation once complete\n",
        "    )\n",
        "\n",
        "    # Decode generated tokens back into text\n",
        "    summary = distilbart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "SwNylV-tS9H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 3: Test DistilBART Summary\n",
        "# =========================\n",
        "\n",
        "sample_text = \"\"\"\n",
        "My favorite book in the Harry Potter series is \"Harry Potter and the Prisoner of Azkaban\". It's a pivotal book in the series, introducing darker themes and complex characters while still maintaining the whimsical charm of the earlier books. The introduction of Sirius Black and Remus Lupin, along with the concept of dementors, adds depth to the wizarding world and Harry's personal journey. The intricate plot, with its time-travel elements and moral ambiguities, keeps the reader guessing until the very end. It's a book that truly showcases J.K. Rowling's storytelling prowess and her ability to blend fantasy with relatable human emotions\n",
        "\"\"\"\n",
        "\n",
        "# Generate summary\n",
        "distilbart_summary = summarize_distilbart(sample_text)\n",
        "\n",
        "# Print output\n",
        "print(\"=== DistilBART Summary ===\")\n",
        "print(distilbart_summary)\n"
      ],
      "metadata": {
        "id": "r6yT_0mzTNWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 1: Load Pegasus Model and Tokenizer\n",
        "# =========================\n",
        "\n",
        "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
        "\n",
        "# Model trained for summarization tasks\n",
        "pegasus_model_name = \"google/pegasus-cnn_dailymail\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(pegasus_model_name)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(pegasus_model_name)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9sV3K2jiDfom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 2: Summarize Text with Pegasus\n",
        "# =========================\n",
        "\n",
        "def summarize_pegasus(text):\n",
        "    # Encode the input text into tokens\n",
        "    inputs = pegasus_tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "    # Generate summary IDs\n",
        "    summary_ids = pegasus_model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=150,        # max length of summary\n",
        "        min_length=40,         # min length of summary\n",
        "        length_penalty=2.0,    # controls length\n",
        "        num_beams=4,           # beam search for better quality\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode generated IDs back into text\n",
        "    summary = pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "vcschIq9E1Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 3: Test Pegasus Summary\n",
        "# =========================\n",
        "\n",
        "sample_text = \"\"\"\n",
        "My favorite book in the Harry Potter series is \"Harry Potter and the Prisoner of Azkaban\". It's a pivotal book in the series, introducing darker themes and complex characters while still maintaining the whimsical charm of the earlier books. The introduction of Sirius Black and Remus Lupin, along with the concept of dementors, adds depth to the wizarding world and Harry's personal journey. The intricate plot, with its time-travel elements and moral ambiguities, keeps the reader guessing until the very end. It's a book that truly showcases J.K. Rowling's storytelling prowess and her ability to blend fantasy with relatable human emotions\n",
        "\"\"\"\n",
        "\n",
        "pegasus_summary = summarize_pegasus(sample_text)\n",
        "\n",
        "print(\"=== Pegasus Summary ===\")\n",
        "print(pegasus_summary)\n"
      ],
      "metadata": {
        "id": "KbjbRpV7E89n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 1: Load T5 Model and Tokenizer\n",
        "# =========================\n",
        "# T5 = \"Text-to-Text Transfer Transformer\"\n",
        "# For summarization, T5 expects the input to start with the prefix: \"summarize: \"\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "t5_model_name = \"t5-base\"   # alternatives: \"t5-small\" (faster), \"t5-large\" (better but heavier)\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name)\n"
      ],
      "metadata": {
        "id": "YSiI9pieNzrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 2: Summarize Text with T5\n",
        "# =========================\n",
        "# Key differences vs BART/Pegasus:\n",
        "# - We prepend \"summarize: \" to the input text (task instruction).\n",
        "# - T5-base typically accepts up to 512 tokens as input (vs ~1024 for BART/Pegasus).\n",
        "\n",
        "def summarize_t5(text):\n",
        "    # Add the task prefix so T5 knows we're summarizing\n",
        "    prefixed_text = \"summarize: \" + text.strip()\n",
        "\n",
        "    # Tokenize/encode; truncate to fit T5's input limits\n",
        "    inputs = t5_tokenizer(\n",
        "        [prefixed_text],\n",
        "        max_length=512,         # T5-base input limit is usually 512 tokens\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Generate the summary token IDs\n",
        "    output_ids = t5_model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=150,         # cap the summary length\n",
        "        min_length=30,          # encourage non-trivial summaries\n",
        "        length_penalty=1.0,     # neutral length penalty for T5\n",
        "        num_beams=4,            # beam search for better quality\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the tokens back to text\n",
        "    summary = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "Cfe8Cog1OTFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 3: Test T5 Summary\n",
        "# =========================\n",
        "\n",
        "sample_text = \"\"\"\n",
        "My favorite book in the Harry Potter series is \"Harry Potter and the Prisoner of Azkaban\". It's a pivotal book in the series, introducing darker themes and complex characters while still maintaining the whimsical charm of the earlier books. The introduction of Sirius Black and Remus Lupin, along with the concept of dementors, adds depth to the wizarding world and Harry's personal journey. The intricate plot, with its time-travel elements and moral ambiguities, keeps the reader guessing until the very end. It's a book that truly showcases J.K. Rowling's storytelling prowess and her ability to blend fantasy with relatable human emotions\n",
        "\"\"\"\n",
        "\n",
        "t5_summary = summarize_t5(sample_text)\n",
        "\n",
        "print(\"=== T5 Summary ===\")\n",
        "print(t5_summary)\n"
      ],
      "metadata": {
        "id": "127dItwVOeXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 1: Compare All 4 Models Side by Side\n",
        "# =========================\n",
        "\n",
        "def compare_models(text):\n",
        "    print(\"=== Input Text ===\")\n",
        "    print(text)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # BART\n",
        "    bart_summary = summarize_text(text)\n",
        "    print(\"=== BART Summary ===\")\n",
        "    print(bart_summary)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # DistilBART\n",
        "    distil_summary = summarize_distilbart(text)\n",
        "    print(\"=== DistilBART Summary ===\")\n",
        "    print(distil_summary)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Pegasus\n",
        "    pegasus_summary = summarize_pegasus(text)\n",
        "    print(\"=== Pegasus Summary ===\")\n",
        "    print(pegasus_summary)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # T5\n",
        "    t5_summary = summarize_t5(text)\n",
        "    print(\"=== T5 Summary ===\")\n",
        "    print(t5_summary)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"BART\": bart_summary,\n",
        "        \"DistilBART\": distil_summary,\n",
        "        \"Pegasus\": pegasus_summary,\n",
        "        \"T5\": t5_summary\n",
        "    }\n",
        "\n",
        "# Example run\n",
        "sample_text = \"\"\"\n",
        "My favorite book in the Harry Potter series is \"Harry Potter and the Prisoner of Azkaban\". It's a pivotal book in the series, introducing darker themes and complex characters while still maintaining the whimsical charm of the earlier books. The introduction of Sirius Black and Remus Lupin, along with the concept of dementors, adds depth to the wizarding world and Harry's personal journey. The intricate plot, with its time-travel elements and moral ambiguities, keeps the reader guessing until the very end. It's a book that truly showcases J.K. Rowling's storytelling prowess and her ability to blend fantasy with relatable human emotions\n",
        "\"\"\"\n",
        "results = compare_models(sample_text)\n"
      ],
      "metadata": {
        "id": "m8c4d4f6pRdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 2a: Install Evaluation Library\n",
        "# =========================\n",
        "!pip install evaluate\n"
      ],
      "metadata": {
        "id": "X8F5X_zBt9wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 2b: Evaluate ROUGE for One Example\n",
        "# =========================\n",
        "!pip install rouge_score\n",
        "import evaluate\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "# Suppose we have a reference (true summary) and a model prediction\n",
        "reference = \"Hugging Face provides pretrained models for NLP tasks like classification, QA, and summarization.\"\n",
        "prediction = results[\"BART\"]   # for example, compare BART‚Äôs output\n",
        "\n",
        "# Compute ROUGE\n",
        "scores = rouge.compute(predictions=[prediction], references=[reference])\n",
        "print(scores)\n"
      ],
      "metadata": {
        "id": "vySPBQett_VA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 3: Compute ROUGE Scores for All Models\n",
        "# =========================\n",
        "\n",
        "import evaluate\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def evaluate_models(text, reference):\n",
        "    # Run all models\n",
        "    outputs = compare_models(text)\n",
        "\n",
        "    scores = {}\n",
        "    for model_name, summary in outputs.items():\n",
        "        # Compute ROUGE for each model vs reference\n",
        "        result = rouge.compute(predictions=[summary], references=[reference])\n",
        "        scores[model_name] = result\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Example reference summary (you can write this yourself, or take it from a dataset)\n",
        "reference_summary = \"Hugging Face provides pretrained models for NLP tasks like classification, question answering, translation, and summarization.\"\n",
        "\n",
        "# Run evaluation\n",
        "scores = evaluate_models(sample_text, reference_summary)\n",
        "\n",
        "# Print results\n",
        "import pprint\n",
        "pprint.pprint(scores)\n"
      ],
      "metadata": {
        "id": "6_aLxYZLup4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 4: Visualize ROUGE Scores\n",
        "# =========================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Convert scores dictionary into a plottable format\n",
        "models = list(scores.keys())   # [\"BART\", \"DistilBART\", \"Pegasus\", \"T5\"]\n",
        "\n",
        "# Extract values for each metric\n",
        "rouge1 = [float(scores[m]['rouge1']) for m in models]\n",
        "rouge2 = [float(scores[m]['rouge2']) for m in models]\n",
        "rougeL = [float(scores[m]['rougeL']) for m in models]\n",
        "\n",
        "x = np.arange(len(models))  # positions for bars\n",
        "width = 0.25  # width of each bar\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(x - width, rouge1, width, label='ROUGE-1')\n",
        "plt.bar(x, rouge2, width, label='ROUGE-2')\n",
        "plt.bar(x + width, rougeL, width, label='ROUGE-L')\n",
        "\n",
        "# Add labels & legend\n",
        "plt.xticks(x, models)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"ROUGE Scores Comparison Across Models\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YJvUlxX9mbV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuning the best performing model"
      ],
      "metadata": {
        "id": "wZLR8QXDsZVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Step 5.1: Install dependencies\n",
        "# ======================\n",
        "!pip install datasets rouge_score transformers accelerate -q\n"
      ],
      "metadata": {
        "id": "M4HAuadxr_Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Step 5.2: Load dataset\n",
        "# ======================\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load small subset to keep training light\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")  # only 1% for demo\n",
        "valid_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:1%]\")\n",
        "\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "id": "XCqL4dR0sBWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# Step 5.3: Tokenization\n",
        "# ======================\n",
        "from transformers import PegasusTokenizer\n",
        "\n",
        "model_name = \"google/pegasus-cnn_dailymail\"\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def preprocess(example):\n",
        "    inputs = tokenizer(\n",
        "        example[\"article\"], truncation=True, padding=\"max_length\", max_length=1024\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        example[\"highlights\"], truncation=True, padding=\"max_length\", max_length=128\n",
        "    )\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_train = dataset.map(preprocess, batched=True)\n",
        "tokenized_valid = valid_dataset.map(preprocess, batched=True)\n"
      ],
      "metadata": {
        "id": "OEbbnqjKsEmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 1 (fixed): Multi-Model Summarizer with 4 models\n",
        "# =========================\n",
        "\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    BartTokenizer, BartForConditionalGeneration,\n",
        "    PegasusTokenizer, PegasusForConditionalGeneration,\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        ")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "MODEL_KEYS = [\n",
        "    \"BART (facebook/bart-large-cnn)\",\n",
        "    \"DistilBART (sshleifer/distilbart-cnn-12-6)\",\n",
        "    \"Pegasus (google/pegasus-xsum)\",\n",
        "    \"T5 (t5-base)\"\n",
        "]\n",
        "\n",
        "MODEL_CONFIGS = {\n",
        "    \"BART (facebook/bart-large-cnn)\": {\n",
        "        \"name\": \"facebook/bart-large-cnn\",\n",
        "        \"tok_cls\": BartTokenizer,\n",
        "        \"mdl_cls\": BartForConditionalGeneration,\n",
        "        \"prefix\": \"\",\n",
        "        \"max_input\": 1024\n",
        "    },\n",
        "    \"DistilBART (sshleifer/distilbart-cnn-12-6)\": {\n",
        "        \"name\": \"sshleifer/distilbart-cnn-12-6\",\n",
        "        \"tok_cls\": BartTokenizer,\n",
        "        \"mdl_cls\": BartForConditionalGeneration,\n",
        "        \"prefix\": \"\",\n",
        "        \"max_input\": 1024\n",
        "    },\n",
        "    \"Pegasus (google/pegasus-xsum)\": {\n",
        "        \"name\": \"google/pegasus-xsum\",\n",
        "        \"tok_cls\": PegasusTokenizer,\n",
        "        \"mdl_cls\": PegasusForConditionalGeneration,\n",
        "        \"prefix\": \"\",\n",
        "        \"max_input\": 1024\n",
        "    },\n",
        "    \"T5 (t5-base)\": {\n",
        "        \"name\": \"t5-base\",\n",
        "        \"tok_cls\": T5Tokenizer,\n",
        "        \"mdl_cls\": T5ForConditionalGeneration,\n",
        "        \"prefix\": \"summarize: \",  # T5 needs explicit instruction\n",
        "        \"max_input\": 512\n",
        "    }\n",
        "}\n",
        "\n",
        "_LOADED = {}\n",
        "\n",
        "def get_model(model_key):\n",
        "    if model_key in _LOADED:\n",
        "        return _LOADED[model_key]\n",
        "    cfg = MODEL_CONFIGS[model_key]\n",
        "    tok = cfg[\"tok_cls\"].from_pretrained(cfg[\"name\"])\n",
        "    mdl = cfg[\"mdl_cls\"].from_pretrained(cfg[\"name\"])\n",
        "    mdl.to(DEVICE)\n",
        "    mdl.eval()\n",
        "    _LOADED[model_key] = (tok, mdl)\n",
        "    return tok, mdl\n",
        "\n",
        "@torch.inference_mode()\n",
        "def summarize_with(model_key, text, max_length=150, min_length=30, num_beams=4, length_penalty=1.0):\n",
        "    cfg = MODEL_CONFIGS[model_key]\n",
        "    tok, mdl = get_model(model_key)\n",
        "    prefixed = (cfg.get(\"prefix\",\"\") + text.strip())\n",
        "    enc = tok([prefixed], max_length=cfg[\"max_input\"], truncation=True, return_tensors=\"pt\")\n",
        "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
        "    out_ids = mdl.generate(\n",
        "        enc[\"input_ids\"],\n",
        "        max_length=int(max_length),\n",
        "        min_length=int(min_length),\n",
        "        num_beams=int(num_beams),\n",
        "        length_penalty=float(length_penalty),\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tok.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def summarize_all(text, max_length=150, min_length=30, num_beams=4, length_penalty=1.0, memory_efficient=True):\n",
        "    outputs = {}\n",
        "    for key in MODEL_KEYS:\n",
        "        outputs[key] = summarize_with(key, text, max_length, min_length, num_beams, length_penalty)\n",
        "        if memory_efficient and key in _LOADED:\n",
        "            del _LOADED[key]\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "kORwicavwEju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 2 (fixed): Gradio UI with 4 models\n",
        "# =========================\n",
        "\n",
        "!pip install gradio evaluate --quiet\n",
        "\n",
        "import gradio as gr\n",
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compare_and_score(text, reference, max_length, min_length, num_beams, length_penalty):\n",
        "    if not text or not text.strip():\n",
        "        empty = [\"\"] * len(MODEL_KEYS)\n",
        "        return (*empty, {})\n",
        "    outputs = summarize_all(\n",
        "        text,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        num_beams=num_beams,\n",
        "        length_penalty=length_penalty,\n",
        "        memory_efficient=True\n",
        "    )\n",
        "    summaries_in_order = [outputs[k] for k in MODEL_KEYS]\n",
        "\n",
        "    if reference and reference.strip():\n",
        "        scores = {}\n",
        "        for k in MODEL_KEYS:\n",
        "            res = rouge.compute(predictions=[outputs[k]], references=[reference])\n",
        "            scores[k] = {m: float(v) for m, v in res.items()}\n",
        "    else:\n",
        "        scores = {}\n",
        "    return (*summaries_in_order, scores)\n",
        "\n",
        "with gr.Blocks(title=\"Multi-Model Text Summarizer\") as demo:\n",
        "    gr.Markdown(\"## üìù Compare 4 Summarization Models (BART, DistilBART, Pegasus, T5)\")\n",
        "    text_in = gr.Textbox(lines=10, label=\"Input Text\", placeholder=\"Paste an article here...\")\n",
        "\n",
        "    with gr.Accordion(\"Advanced Settings\", open=False):\n",
        "        max_len = gr.Slider(40, 300, value=150, step=10, label=\"max_length\")\n",
        "        min_len = gr.Slider(10, 150, value=30, step=5, label=\"min_length\")\n",
        "        beams = gr.Slider(1, 8, value=4, step=1, label=\"num_beams\")\n",
        "        lpen = gr.Slider(0.1, 2.5, value=1.0, step=0.1, label=\"length_penalty\")\n",
        "\n",
        "    ref_in = gr.Textbox(lines=3, label=\"(Optional) Reference Summary for ROUGE\")\n",
        "\n",
        "    run_btn = gr.Button(\"Compare Models\")\n",
        "\n",
        "    out_bart       = gr.Textbox(label=MODEL_KEYS[0])\n",
        "    out_distilbart = gr.Textbox(label=MODEL_KEYS[1])\n",
        "    out_pegasus    = gr.Textbox(label=MODEL_KEYS[2])\n",
        "    out_t5         = gr.Textbox(label=MODEL_KEYS[3])\n",
        "    metrics_json   = gr.JSON(label=\"ROUGE Scores\")\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=compare_and_score,\n",
        "        inputs=[text_in, ref_in, max_len, min_len, beams, lpen],\n",
        "        outputs=[out_bart, out_distilbart, out_pegasus, out_t5, metrics_json]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "2RO8aPacuzY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AX0w-A4ewioZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ======================\n",
        "# # Step 5.4: Fine-tune model (with W&B disabled)\n",
        "# # ======================\n",
        "# import os\n",
        "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# from transformers import PegasusForConditionalGeneration, Trainer, TrainingArguments\n",
        "\n",
        "# model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     eval_strategy=\"epoch\",\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=2,\n",
        "#     per_device_eval_batch_size=2,\n",
        "#     num_train_epochs=1,  # keep small for demo\n",
        "#     weight_decay=0.01,\n",
        "#     save_total_limit=1,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_steps=10,\n",
        "#     push_to_hub=False,\n",
        "#     report_to=\"none\"  # also disables other loggers\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tokenized_train,\n",
        "#     eval_dataset=tokenized_valid,\n",
        "# )\n",
        "\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "id": "UDeR9WsgsHm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the requirements.txt file\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(\"transformers>=4.30.0\\n\")\n",
        "    f.write(\"torch\\n\")\n",
        "    f.write(\"gradio\\n\")\n",
        "    f.write(\"evaluate\\n\")\n"
      ],
      "metadata": {
        "id": "r9IlwuOXxNE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# readme_text = \"\"\"\n",
        "# ---\n",
        "# title: Multi-Model Text Summarizer\n",
        "# emoji: üìù\n",
        "# colorFrom: blue\n",
        "# colorTo: green\n",
        "# sdk: gradio\n",
        "# sdk_version: \"3.42\"\n",
        "# app_file: app.py\n",
        "# pinned: true\n",
        "# ---\n",
        "\n",
        "\n",
        "# # üìù Multi-Model Text Summarizer\n",
        "\n",
        "\n",
        "# This project compares **4 state-of-the-art summarization models** using Hugging Face Transformers and Gradio:\n",
        "\n",
        "# - **BART (facebook/bart-large-cnn)**\n",
        "# - **DistilBART (sshleifer/distilbart-cnn-12-6)**\n",
        "# - **Pegasus (google/pegasus-xsum)**\n",
        "# - **T5 (t5-base)**\n",
        "\n",
        "# ## üöÄ Features\n",
        "# - Input any long text (e.g., articles, reports).\n",
        "# - Generate summaries from all 4 models.\n",
        "# - Compare results side by side.\n",
        "# - (Optional) Provide a reference summary and compute **ROUGE scores**.\n",
        "\n",
        "# ## üñ•Ô∏è Tech Stack\n",
        "# - Python\n",
        "# - Hugging Face Transformers\n",
        "# - PyTorch\n",
        "# - Gradio (for UI)\n",
        "# - Evaluate (for metrics)\n",
        "\n",
        "# ## üì¶ Setup\n",
        "# To run locally:\n",
        "\n",
        "#     pip install -r requirements.txt\n",
        "#     python app.py\n",
        "\n",
        "# ## üåê Deployment\n",
        "# This app can run locally or be deployed on **Hugging Face Spaces**.\n",
        "# \"\"\"\n",
        "\n",
        "# with open(\"README.md\", \"w\") as f:\n",
        "#     f.write(readme_text)\n"
      ],
      "metadata": {
        "id": "Ve1TQDLGxrOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "readme_text = \"\"\"\n",
        "---\n",
        "title: Multi-Model Text Summarizer\n",
        "emoji: üìù\n",
        "colorFrom: blue\n",
        "colorTo: green\n",
        "sdk: gradio\n",
        "app_file: app.py\n",
        "pinned: true\n",
        "---\n",
        "\n",
        "# üìù Multi-Model Text Summarizer\n",
        "\n",
        "This project compares **4 state-of-the-art summarization models** using Hugging Face Transformers and Gradio:\n",
        "\n",
        "- **BART (facebook/bart-large-cnn)**\n",
        "- **DistilBART (sshleifer/distilbart-cnn-12-6)**\n",
        "- **Pegasus (google/pegasus-xsum)**\n",
        "- **T5 (t5-base)**\n",
        "\n",
        "## üöÄ Features\n",
        "- Input any long text (e.g., articles, reports).\n",
        "- Generate summaries from all 4 models.\n",
        "- Compare results side by side.\n",
        "- (Optional) Provide a reference summary and compute **ROUGE scores**.\n",
        "\n",
        "## üñ•Ô∏è Tech Stack\n",
        "- Python\n",
        "- Hugging Face Transformers\n",
        "- PyTorch\n",
        "- Gradio (for UI)\n",
        "- Evaluate (for metrics)\n",
        "\n",
        "## üì¶ Setup\n",
        "To run locally:\n",
        "\n",
        "    pip install -r requirements.txt\n",
        "    python app.py\n",
        "\n",
        "## üåê Deployment\n",
        "This app can run locally or be deployed on **Hugging Face Spaces**.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(readme_text)\n"
      ],
      "metadata": {
        "id": "qh6-7YwtC4xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# app.py - Main Gradio UI\n",
        "# ======================\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, PegasusForConditionalGeneration, PegasusTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# ----------------------\n",
        "# Load Models & Tokenizers\n",
        "# ----------------------\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "distilbart_tokenizer = BartTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "distilbart_model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "# ----------------------\n",
        "# Helper function for summarization\n",
        "# ----------------------\n",
        "def summarize(text, model, tokenizer, prefix=\"\"):\n",
        "    inputs = tokenizer(\n",
        "        [prefix + text],\n",
        "        max_length=1024,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True\n",
        "    )\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=150,\n",
        "        min_length=40,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ----------------------\n",
        "# Gradio App\n",
        "# ----------------------\n",
        "def multi_model_summarizer(text):\n",
        "    return {\n",
        "        \"BART\": summarize(text, bart_model, bart_tokenizer),\n",
        "        \"DistilBART\": summarize(text, distilbart_model, distilbart_tokenizer),\n",
        "        \"Pegasus\": summarize(text, pegasus_model, pegasus_tokenizer),\n",
        "        \"T5\": summarize(text, t5_model, t5_tokenizer, prefix=\"summarize: \")\n",
        "    }\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=multi_model_summarizer,\n",
        "    inputs=gr.Textbox(lines=8, placeholder=\"Paste text here...\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"BART Summary\"),\n",
        "        gr.Textbox(label=\"DistilBART Summary\"),\n",
        "        gr.Textbox(label=\"Pegasus Summary\"),\n",
        "        gr.Textbox(label=\"T5 Summary\")\n",
        "    ],\n",
        "    title=\"üìù Multi-Model Text Summarizer\",\n",
        "    description=\"Compare summaries from BART, DistilBART, Pegasus, and T5.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ],
      "metadata": {
        "id": "dkvcGmD0x11Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_code = \"\"\"# ======================\n",
        "# app.py - Main Gradio UI\n",
        "# ======================\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, PegasusForConditionalGeneration, PegasusTokenizer, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# ----------------------\n",
        "# Load Models & Tokenizers\n",
        "# ----------------------\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "distilbart_tokenizer = BartTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "distilbart_model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "# ----------------------\n",
        "# Helper function for summarization\n",
        "# ----------------------\n",
        "def summarize(text, model, tokenizer, prefix=\"\"):\n",
        "    inputs = tokenizer(\n",
        "        [prefix + text],\n",
        "        max_length=1024,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True\n",
        "    )\n",
        "    summary_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=150,\n",
        "        min_length=40,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ----------------------\n",
        "# Gradio App\n",
        "# ----------------------\n",
        "def multi_model_summarizer(text):\n",
        "    return (\n",
        "        summarize(text, bart_model, bart_tokenizer),\n",
        "        summarize(text, distilbart_model, distilbart_tokenizer),\n",
        "        summarize(text, pegasus_model, pegasus_tokenizer),\n",
        "        summarize(text, t5_model, t5_tokenizer, prefix=\"summarize: \")\n",
        "    )\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=multi_model_summarizer,\n",
        "    inputs=gr.Textbox(lines=8, placeholder=\"Paste text here...\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"BART Summary\"),\n",
        "        gr.Textbox(label=\"DistilBART Summary\"),\n",
        "        gr.Textbox(label=\"Pegasus Summary\"),\n",
        "        gr.Textbox(label=\"T5 Summary\")\n",
        "    ],\n",
        "    title=\"üìù Multi-Model Text Summarizer\",\n",
        "    description=\"Compare summaries from BART, DistilBART, Pegasus, and T5.\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n",
        "\"\"\"\n",
        "\n",
        "# write to file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n"
      ],
      "metadata": {
        "id": "hA-1x0W2ymew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub --quiet\n",
        "from huggingface_hub import login\n",
        "\n",
        "# This will ask you for your Hugging Face token\n",
        "login()\n"
      ],
      "metadata": {
        "id": "HH2oORye_Moa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/spaces/priankaM/multi-model-text-summarizer\n"
      ],
      "metadata": {
        "id": "Cd4ca3HJAUPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/app.py /content/multi-model-text-summarizer/\n",
        "!cp /content/requirements.txt /content/multi-model-text-summarizer/\n",
        "!cp /content/README.md /content/multi-model-text-summarizer/\n"
      ],
      "metadata": {
        "id": "xT4RSdrIAiQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/multi-model-text-summarizer/\n",
        "!git add .\n",
        "!git commit -m \"Initial commit of multi-model summarizer\"\n",
        "!git push\n"
      ],
      "metadata": {
        "id": "mg2CkMyNArGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"your_email@example.com\"\n",
        "!git config --global user.name \"Y\"\n"
      ],
      "metadata": {
        "id": "3fZnH8W9BEoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add README.md\n",
        "!git commit -m \"Fix Space metadata YAML\"\n",
        "!git push\n"
      ],
      "metadata": {
        "id": "GqF-YEBOCLWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/multi-model-text-summarizer\n"
      ],
      "metadata": {
        "id": "FB0IST_AH8bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git init\n"
      ],
      "metadata": {
        "id": "ZfrWy_CPH-O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"your-email@example.com\"\n",
        "!git config --global user.name \"your-username\"\n"
      ],
      "metadata": {
        "id": "v0bs7QR7IDO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://huggingface.co/spaces/priankaM/multi-model-text-summarizer\n"
      ],
      "metadata": {
        "id": "sUclHkyZIHGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"Initial commit: multi-model summarizer placeholders\"\n"
      ],
      "metadata": {
        "id": "-NekY21PIKjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main\n"
      ],
      "metadata": {
        "id": "srWPZ3zyINMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save app.py\n",
        "app_code = \"\"\"\n",
        "import gradio as gr\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, PegasusForConditionalGeneration, T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Load models and tokenizers here (we won't run them locally, just define)\n",
        "# ...\n",
        "\n",
        "def summarize_text(text):\n",
        "    # Dummy function for Space; actual inference happens in the Space\n",
        "    return \"Summary placeholder\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=summarize_text,\n",
        "    inputs=gr.Textbox(lines=10, placeholder=\"Paste text here...\"),\n",
        "    outputs=[gr.Textbox(label=\"BART Summary\"),\n",
        "             gr.Textbox(label=\"DistilBART Summary\"),\n",
        "             gr.Textbox(label=\"Pegasus Summary\"),\n",
        "             gr.Textbox(label=\"T5 Summary\")],\n",
        "    title=\"Multi-Model Text Summarizer\",\n",
        "    description=\"Compare summaries from 4 models\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch()\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "\n",
        "# Save README.md with correct YAML front matter\n",
        "readme_text = \"\"\"---\n",
        "title: Multi-Model Text Summarizer\n",
        "emoji: üìù\n",
        "colorFrom: blue\n",
        "colorTo: green\n",
        "sdk: gradio\n",
        "app_file: app.py\n",
        "pinned: true\n",
        "---\n",
        "\n",
        "# Multi-Model Text Summarizer\n",
        "Compare 4 state-of-the-art summarization models in Hugging Face Spaces.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(readme_text)\n",
        "\n",
        "# Save requirements.txt\n",
        "requirements = \"\"\"gradio\n",
        "transformers\n",
        "torch\n",
        "\"\"\"\n",
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements)\n"
      ],
      "metadata": {
        "id": "to-S55qiHcYr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}